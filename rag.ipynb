{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd522c5f-14fc-4796-9076-adc1bb7ec292",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350076a-b4bb-42e6-91f7-dea43eae5d2b",
   "metadata": {},
   "source": [
    "#### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df765fe1-d89c-4dc8-a55e-1f2fbaf343ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36de0-2cb2-4283-a97c-626b10fff892",
   "metadata": {},
   "source": [
    "#### Anonymizing Emails & Phone No.s on Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56138326-e84f-43e9-8047-a023c9ebb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "def anonymize_resume(text):\n",
    "    text = re.sub(\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'EMAIL_REDACTED',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    phone_pattern = re.compile(\n",
    "        r'(\\+?\\d{1,3}[\\s.-]?)?'\n",
    "        r'(\\(?\\d{3}\\)?[\\s.-]?)'           \n",
    "        r'\\d{3}[\\s.-]?\\d{4}'              \n",
    "    )\n",
    "    \n",
    "    text = re.sub(\n",
    "        r'(\\+?\\d{1,3}[\\s.-]?)?(\\(?\\d{3}\\)?[\\s.-]?)\\d{3}\\s*[\\-.\\s]\\s*\\d{4}',\n",
    "        'PHONE_REDACTED',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    text = re.sub(r'https?://\\S+', 'URL_REDACTED', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b7eca-3965-45c6-b802-d250db97de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = open(\"SUPER RESUME.pdf\", \"rb\")\n",
    "reader = PyPDF2.PdfReader(pdf_file)\n",
    "resume_text = \"\"\n",
    "for page in reader.pages:\n",
    "    resume_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "pdf_file.close()\n",
    "\n",
    "safe_text = anonymize_resume(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494245ec-e6d3-451c-97ec-067ee1dc849b",
   "metadata": {},
   "source": [
    "#### Extracting metadata from resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f062c084-5d39-4236-a69c-523fc051d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract the following metadata from this anonymized resume as JSON:\n",
    "- skills\n",
    "- projects\n",
    "- tech_stack\n",
    "- tags\n",
    "- experience_level\n",
    "- education\n",
    "\n",
    "Resume text:\n",
    "{safe_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63778867-6e67-426e-8338-8ca6ffc07505",
   "metadata": {},
   "source": [
    "#### Pydantic Model for Resume metadata and its parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170f6cb7-746d-4ca8-9a03-bd03e22efb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\llm-langchain\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class ResumeMetadata(BaseModel):\n",
    "    skills: Optional[List[str]] = None\n",
    "    projects: Optional[List[str]] = None\n",
    "    tech_stack: Optional[List[str]] = None\n",
    "    tags: Optional[List[str]] = None\n",
    "    experience_level: Optional[str] = None\n",
    "    education: Optional[List[Union[str, dict]]] = None\n",
    "\n",
    "metadata_parser = PydanticOutputParser(pydantic_object=ResumeMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdb656-5ed5-4f60-8638-8d6250f3c6d0",
   "metadata": {},
   "source": [
    "#### Converting Metadata object into string for Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8afbe25-7e1a-4789-a909-bcec57377bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metadata_for_chroma(metadata: ResumeMetadata) -> dict:\n",
    "    def list_to_str(lst):\n",
    "        if not lst:\n",
    "            return \"\"\n",
    "        out = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, dict):\n",
    "                degree = item.get(\"degree\", \"\")\n",
    "                institution = item.get(\"institution\", \"\")\n",
    "                duration = item.get(\"duration\", \"\")\n",
    "                out.append(f\"{degree} at {institution} ({duration})\".strip())\n",
    "            else:\n",
    "                out.append(str(item))\n",
    "        return \", \".join(out)\n",
    "\n",
    "    return {\n",
    "        \"skills\": list_to_str(metadata.skills),\n",
    "        \"projects\": list_to_str(metadata.projects),\n",
    "        \"tech_stack\": list_to_str(metadata.tech_stack),\n",
    "        \"tags\": list_to_str(metadata.tags),\n",
    "        \"experience_level\": metadata.experience_level or \"\",\n",
    "        \"education\": list_to_str(metadata.education)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abeee6f1-e6b9-4ed0-9c7a-e6fd256626da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_18564\\3623507732.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_18564\\3623507732.py:3: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(f\"Extract metadata from the resume text:\\n{safe_text}\\n{parser.get_format_instructions()}\")\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "response = llm.predict(f\"Extract metadata from the resume text:\\n{safe_text}\\n{parser.get_format_instructions()}\")\n",
    "metadata = metadata_parser.parse(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483606fe-dd8b-460e-9909-7d6427319e54",
   "metadata": {},
   "source": [
    "#### Splitting Resume data into chunks, making documents and creating the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd42ad0-fe05-41e2-b025-8940260a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(safe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed23bc5-c3ad-4c78-8a0b-b4aa2e224de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "metadata_dict = convert_metadata_for_chroma(metadata)  \n",
    "documents = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    doc = Document(\n",
    "        page_content=chunk,\n",
    "        metadata=metadata_dict\n",
    "    )\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193ba84e-37d2-4039-aa61-e31edfc1c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_18564\\2770405346.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e64aed7-d23f-4e27-9947-0f63b4233734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_db = Chroma.from_documents(documents, embeddings, collection_name=\"resumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd5f21-d957-4637-8f55-227b96acbcc7",
   "metadata": {},
   "source": [
    "#### Basic prompt for RAG with question and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "992491bb-3af4-46ad-9176-3700b031fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Use the following resume information to answer the user's question.\n",
    "\n",
    "Resume Information:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a concise and professional manner.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af3f750-2f30-4c82-9ffa-d724b37322b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_18564\\3097457791.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bbcc5-bd45-4c10-963f-816a18f5cada",
   "metadata": {},
   "source": [
    "#### Simple Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437cd85b-a18f-4016-8149-e98b9a092e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\" \n",
    "About the job\n",
    "Inclusion without Exception:\n",
    "\n",
    "Tata Consultancy Services (TCS) is an equal opportunity employer, and embraces diversity in race, nationality, ethnicity, gender, age, physical ability, neurodiversity, and sexual orientation, to create a workforce that reflects the societies we operate in. Our continued commitment to Culture and Diversity is reflected in our people stories across our workforce and implemented through equitable workplace policies and processes.\n",
    "\n",
    "TCS is an IT services, consulting, and business solutions organization that has been partnering with many of the world’s largest businesses in their transformation journeys for over 55 years. Its consulting-led, cognitive-powered portfolio of business, technology, and engineering services and solutions is delivered through its unique Location Independent Agile™ delivery model, recognized as a benchmark of excellence in software development. A part of the Tata group, India's largest multinational business group, TCS operates in 55 countries and employs over 612607,000 of the world’s best-trained consultants highly skilled individuals in 55 countries, including more than 10,000 in Canada. The company generated consolidated revenues of US $29 30 billion in the fiscal year ended March 31, 20254 and is listed on the BSE and the NSE in India. TCS' proactive stance on climate change and award-winning work with communities across the world have earned it a place in leading sustainability indices such as the MSCI Global Sustainability Index and the FTSE4Good Emerging Index.\n",
    "\n",
    "\n",
    "Skills and Responsibilities:\n",
    "\n",
    "•Must Have Technical Functional Skills Artificial Intelligence, Machine Learning Data Science professional with experience in enterprise architecture, AI product security, and digital transformation. \n",
    "\n",
    "•Strong hands-on expertise in cloud-native architecture (AWSGCP), threat modeling, secure AI system development, and AI governance. \n",
    "\n",
    "•Led mission-critical initiatives with cross-functional teams, aligning product innovation with cybersecurity best practices and compliance standards.\n",
    "\n",
    "\n",
    "Roles Responsibilities-\n",
    "\n",
    "•Strong Knowledge on AIML security, Web, API, JS Security, Mobile, Cloud Security-Engineer An Engineer with proven strong technical competence in building and implementing in AIML. \n",
    "\n",
    "•You will be part of a fast-paced team responsible for developing and delivering products and components focused on application security. \n",
    "\n",
    "•You will be challenged with identifying innovative ideas and proof of concepts to deliver against the existing and future needs. \n",
    "\n",
    "•Strong AI ML Engineer-Security Architecture-Development Experience Web, API, Mobile, Full Stack\n",
    "•This role primarily involves performing DevOps activity in the team which manages high scalable enterprise applications. \n",
    "\n",
    "•You will be working on integrating off-the-shelf Application Security solutions into the environment. -OWASP, ASVS, NIST\n",
    " \n",
    "Tata Consultancy Services Canada Inc. is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please inform Human Resources.\n",
    " \n",
    "Thank you for your interest in TCS. Candidates that meet the qualifications for this position will be contacted within a 2-week period. We invite you to continue to apply for other opportunities that match your profile.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58aadaa1-3d2a-4ff2-aa4a-6b6ae9088b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_18564\\2226263329.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm_chain.run(context=llm_input, question=query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your resume information, you appear to be a good fit for the job description at Tata Consultancy Services (TCS). Your experience in Artificial Intelligence and Machine Learning aligns well with the required technical skills, particularly in areas such as AI product security, cloud-native architecture, and data science. Your hands-on expertise with Python, TensorFlow, PyTorch, and cloud platforms like Google Cloud aligns with TCS's focus on innovative solutions and digital transformation.\n",
      "\n",
      "Additionally, your projects demonstrate a strong background in developing scalable applications and working with real-time data, which is essential for the role. Your knowledge of security practices in AI and your experience with cross-functional collaboration further enhance your suitability for the position. \n",
      "\n",
      "Overall, you possess the relevant skills and experience that match the responsibilities outlined in the job description, making you a promising candidate for this role.\n"
     ]
    }
   ],
   "source": [
    "query = \"Am i a good fit for this job desc?\"\n",
    "\n",
    "context_text = vector_db.similarity_search(query, k=3)  # top 3 matching documents\n",
    "\n",
    "if job_description.strip():  # JD is not empty\n",
    "    llm_input = f\"Job Description:\\n{job_description}\\n\\nResume Info:\\n{context_text}\"\n",
    "else: \n",
    "    llm_input = f\"Resume Info:\\n{context_text}\"\n",
    "    \n",
    "response = llm_chain.run(context=llm_input, question=query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0afc7952-d253-403b-9605-fc517b6bff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enhance your resume for IT services roles focused on AI and machine learning, consider undertaking a project that integrates security measures with AI applications. Here are a few project ideas:\n",
      "\n",
      "1. **AI-Powered Cyber Threat Detection**: Develop a system that uses machine learning to analyze network traffic and detect anomalies indicative of cyber threats. This could involve using supervised learning techniques to classify normal versus malicious traffic.\n",
      "\n",
      "2. **Secure AI Model Deployment**: Create a project that focuses on deploying a machine learning model with emphasis on security best practices. You could explore topics like model encryption, secure APIs, and access management.\n",
      "\n",
      "3. **Privacy-Preserving Machine Learning**: Implement a project that uses federated learning or differential privacy techniques to enable machine learning without compromising user data privacy.\n",
      "\n",
      "4. **Vulnerability Scanning Tool**: Build a web application that scans for common vulnerabilities in web applications using AI to prioritize and suggest remediation strategies based on risk levels.\n",
      "\n",
      "5. **AI-Enhanced Application Security Monitoring**: Create a tool that utilizes machine learning to analyze logs and detect potential security incidents in real-time, possibly integrating with existing security frameworks like OWASP.\n",
      "\n",
      "These projects will not only demonstrate your technical skills but also your understanding of the importance of security in AI and machine learning systems, making you a more attractive candidate for relevant positions.\n"
     ]
    }
   ],
   "source": [
    "query = \"Apart from the projects in my resume, what other project can i do that might be good for my resume for these types of jobs?\"\n",
    "\n",
    "context_text = vector_db.similarity_search(query, k=3)  # top 3 matching documents\n",
    "\n",
    "if job_description.strip():  # JD is not empty\n",
    "    llm_input = f\"Job Description:\\n{job_description}\\n\\nResume Info:\\n{context_text}\"\n",
    "else: \n",
    "    llm_input = f\"Resume Info:\\n{context_text}\"\n",
    "    \n",
    "response = llm_chain.run(context=llm_input, question=query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b91775-977e-4769-a87a-7ab7b59ac207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3bee7-1417-4898-bd37-52c5956af593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60bab77-f5a2-49a6-a90b-abd2b5b7e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\llm-langchain\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3667: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac1ae38-4b66-44b1-bfec-66b3a6442f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_summary = f\"\"\"\n",
    "Projects: {metadata_dict['projects']}\n",
    "Skills: {metadata_dict['skills']}\n",
    "Tech Stack: {metadata_dict['tech_stack']}\n",
    "Tags: {metadata_dict['tags']}\n",
    "Experience Level: {metadata_dict['experience_level']}\n",
    "Education: {metadata_dict['education']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf3950-36da-4b64-9f3b-16b1ebea79f2",
   "metadata": {},
   "source": [
    "#### Summarizing Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f969fee1-7dbc-464e-b064-2c29d8103a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Job Title:** AI/ML Security Engineer\n",
      "\n",
      "**Company:** Tata Consultancy Services (TCS)\n",
      "\n",
      "**Location:** Canada\n",
      "\n",
      "**About TCS:**\n",
      "TCS is an IT services, consulting, and business solutions organization with over 55 years of experience, operating in 55 countries. The company focuses on transformation journeys for large businesses and emphasizes diversity and sustainability.\n",
      "\n",
      "**Key Responsibilities:**\n",
      "- Develop and deliver products focused on application security.\n",
      "- Identify innovative ideas and proof of concepts to meet current and future needs.\n",
      "- Perform DevOps activities for high-scalable enterprise applications.\n",
      "- Integrate off-the-shelf Application Security solutions (e.g., OWASP, ASVS, NIST).\n",
      "\n",
      "**Required Skills:**\n",
      "- Expertise in Artificial Intelligence (AI), Machine Learning (ML), and Data Science.\n",
      "- Strong hands-on experience in cloud-native architecture (AWS, GCP).\n",
      "- Knowledge of AIML security, Web, API, JavaScript, Mobile, and Cloud Security.\n",
      "- Experience in AI product security and digital transformation.\n",
      "- Proven ability to lead cross-functional teams and align product innovation with cybersecurity best practices.\n",
      "\n",
      "**Qualifications:**\n",
      "- Strong technical competence in building and implementing AI/ML solutions.\n",
      "- Experience in security architecture and development for Web, API, Mobile, and Full Stack applications.\n",
      "\n",
      "**Additional Information:**\n",
      "- TCS is committed to accessibility and will provide accommodations during the recruitment process if needed. \n",
      "\n",
      "**Application Process:**\n",
      "Candidates meeting the qualifications will be contacted within two weeks.\n"
     ]
    }
   ],
   "source": [
    "jd_summary = []\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that processes job descriptions. \n",
    "Your task is to keep all important and relevant information (skills, technologies, responsibilities, qualifications, and role-specific details) \n",
    "and remove anything unnecessary, repetitive, or generic.\n",
    "\n",
    "Here is the job description:\n",
    "{job_description}\n",
    "\n",
    "Return a clean, concise summary that captures only the essential information for understanding the role and matching a resume.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "chain = summarize_prompt | llm\n",
    "jd_summary = chain.invoke({\"job_description\": job_description})\n",
    "print(jd_summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b79ce-b3ba-4b3e-8262-beb0b534e6b8",
   "metadata": {},
   "source": [
    "#### Routing to either vector store, web or llm based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6dbe0da-21d3-4e3f-9452-a14298f3d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "routing_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_question\", \"metadata_summary\", \"jd_summary\"],\n",
    "    template=\"\"\"\n",
    "You are an expert assistant who decides where to route a user's question.\n",
    "\n",
    "The user's resume contains information about the following:\n",
    "{metadata_summary}\n",
    "\n",
    "The user also provided a job description summary:\n",
    "{jd_summary}\n",
    "\n",
    "Routing rules (priority order):\n",
    "1. If the question is about the user's resume (projects, skills, experience, education) \n",
    "   or how the resume relates to the job description → respond with VECTORSTORE.\n",
    "2. If the question is about very recent or ongoing current events (e.g., 2025 or later), \n",
    "   live news, or real-time trends → respond with WEBSEARCH.\n",
    "3. Otherwise (including historical events, general knowledge, greetings, or casual conversation) → respond with LLM.\n",
    "\n",
    "Respond ONLY with one of: VECTORSTORE, WEBSEARCH, or LLM.\n",
    "\n",
    "User Question: {user_question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "routing_chain = LLMChain(llm=llm, prompt=routing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5640e028-ad70-4bae-af1d-e08dc862425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTED TO: LLM\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "route = routing_chain.run({\n",
    "    \"user_question\": \"Who won the olympics recently?\",\n",
    "    \"metadata_summary\": metadata_summary,\n",
    "    \"jd_summary\": jd_summary if jd_summary else \"No job description provided.\"\n",
    "})\n",
    "\n",
    "print(\"ROUTED TO:\", route.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac996266-3777-45aa-ba72-8718c81bfc2e",
   "metadata": {},
   "source": [
    "### LangFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "082c5b71-489e-4485-9040-d36a388cc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TAVILY_API_KEY'] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed79c9-f9f5-409f-80ce-a3add57a984a",
   "metadata": {},
   "source": [
    "#### Basic RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41f54c4e-c2ee-47d7-bd7c-aac9a2cfc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Use the following resume information to answer the user's question.\n",
    "\n",
    "Resume Information:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a concise and professional manner.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "747e33ff-6c46-4f1c-8d57-121e10042d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478fd8b2-197e-46fc-b728-c2c5dbb994a8",
   "metadata": {},
   "source": [
    "#### Functions for nodes in our Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5e905c9a-ea97-4c2f-a10d-29042c62d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"Retrieve from vectorstore (resume info)\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = vector_db.similarity_search(question)\n",
    "    return {\"question\": question, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f91b252-08f1-4961-a924-f14325372812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_docs(state):\n",
    "    \"\"\"Generate answer using top resume matches + optional JD\"\"\"\n",
    "    query = state[\"question\"]\n",
    "    \n",
    "    # Retrieve top 3 matching docs from vectorstore\n",
    "    context_docs = vector_db.similarity_search(query, k=3)\n",
    "    context_text = \"\\n\".join([d.page_content for d in context_docs])\n",
    "    \n",
    "    jd = state.get(\"job_description\", \"\")\n",
    "    if hasattr(jd, \"content\"):  # AIMessage -> get the string\n",
    "        jd = jd.content\n",
    "    jd = jd.strip() if jd else \"\"\n",
    "\n",
    "    # Build LLM input\n",
    "    if jd:\n",
    "        llm_input = f\"Job Description:\\n{jd}\\n\\nResume Info:\\n{context_text}\"\n",
    "    else:\n",
    "        llm_input = f\"Resume Info:\\n{context_text}\"\n",
    "    \n",
    "    # Run LLM chain\n",
    "    response = llm_chain.run(context=llm_input, question=query)\n",
    "    \n",
    "    return {\"question\": query, \"generation\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "988a8030-0880-4e1d-a8bf-4b2f2f52592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_fallback(state):\n",
    "    \"\"\"LLM fallback for greetings/general questions\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    generation = llm.predict(f\"Answer the question/ greeting/ small talk concisely : {question}\")\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"Web search for live events/trends\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Context (from web): {web_results}\n",
    "\n",
    "    Answer concisely.\n",
    "    \"\"\"\n",
    "    generation = llm.predict(prompt)\n",
    "    return {\"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31139e64-e966-4388-83bb-a430896e0306",
   "metadata": {},
   "source": [
    "#### Defining GrapgState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5889cbb4-634b-4b3d-8e6f-f624b851dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"|\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    job_description: str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d26fcd-8e24-4fe7-b6a8-2235f583a540",
   "metadata": {},
   "source": [
    "#### Pydantic model and parser for Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5a15675b-53f7-4434-b024-ad2b8b5bb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question: 'yes' or 'no'\"\n",
    "    )\n",
    "    hallucination: str = Field(\n",
    "        description=\"Does the answer contain hallucinations or information not in the context? 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=GradeAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1412f51d-3732-4367-b99e-6f409028b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    context = \"\\n\".join([d.page_content for d in state.get(\"documents\", [])])  # optional\n",
    "    \n",
    "    prompt_text = f\"\"\"\n",
    "You are a grader assessing an AI answer.\n",
    "\n",
    "1. Check if the answer addresses/resolves the user's question.  \n",
    "2. Check if the answer contains hallucinations (information not present in the context/resume provided).  \n",
    "\n",
    "Respond ONLY in JSON using this schema:\n",
    "{parser.get_format_instructions()}\n",
    "\n",
    "User Question: {question}\n",
    "AI Answer: {generation}\n",
    "Context (resume/docs): {context}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.predict(prompt_text)\n",
    "    graded = parser.parse(response)\n",
    "\n",
    "    if graded.binary_score.lower() == \"yes\" and graded.hallucination.lower() == \"no\":\n",
    "        return \"useful\"\n",
    "    elif graded.binary_score.lower() == \"yes\" and graded.hallucination.lower() == \"yes\":\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6013c33e-6296-4981-9393-2d905d678f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "routing_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_question\", \"metadata_summary\", \"jd_summary\"],\n",
    "    template=\"\"\"\n",
    "You are an expert assistant who decides where to route a user's question.\n",
    "\n",
    "The user's resume contains information about the following:\n",
    "{metadata_summary}\n",
    "\n",
    "The user also provided a job description summary:\n",
    "{jd_summary}\n",
    "\n",
    "Routing rules (priority order):\n",
    "1. If the question is about the user's resume (projects, skills, experience, education) \n",
    "   or how the resume relates to the job description → respond with VECTORSTORE.\n",
    "2. If the question is about very recent or ongoing current events (e.g., 2025 or later), \n",
    "   live news, or real-time trends → respond with WEBSEARCH.\n",
    "3. Otherwise (including historical events, general knowledge, greetings, or casual conversation) → respond with LLM.\n",
    "\n",
    "Respond ONLY with one of: VECTORSTORE, WEBSEARCH, or LLM.\n",
    "\n",
    "User Question: {user_question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "routing_chain = LLMChain(llm=llm, prompt=routing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c087607e-8f45-4915-bb89-e15730ddd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    question = state[\"question\"].lower()\n",
    "    jd_summary = state.get(\"job_description\", \"\")\n",
    "    if hasattr(jd_summary, \"content\"):\n",
    "        jd_summary = jd_summary.content\n",
    "    jd_summary = jd_summary.lower() if jd_summary else \"\"\n",
    "\n",
    "    if jd_summary and any(word in question for word in [\"company\", \"role\", \"position\", \"salary\", \"location\"]):\n",
    "        return \"retrieve\"  # VECTORSTORE / generate_from_docs node\n",
    "\n",
    "    # Otherwise use LLM chain routing\n",
    "    route = routing_chain.run({\n",
    "        \"user_question\": state[\"question\"],\n",
    "        \"metadata_summary\": state.get(\"metadata_summary\", \"\"),\n",
    "        \"jd_summary\": state.get(\"job_description\", \"\")\n",
    "    }).strip().lower()\n",
    "\n",
    "    if route == \"vectorstore\":\n",
    "        return \"retrieve\"\n",
    "    elif route == \"websearch\":\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"llm_fallback\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4c9f2b7d-805a-4432-a417-0404237285b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START \n",
    "\n",
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7448259b-a254-4ce7-9a4c-8504927dc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate_from_docs)\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"web_search\",\n",
    "        \"not supported\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "510650ae-1f0a-4ec8-95c6-58528e2ff3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('To better align your resume with the AI/ML Security Engineer position at '\n",
      " 'Tata Consultancy Services, consider the following changes:\\n'\n",
      " '\\n'\n",
      " '1. **Highlight Relevant Experience:**\\n'\n",
      " '   - Emphasize any experience related to application security, particularly '\n",
      " 'in the context of AI/ML. If you have worked on security aspects of your '\n",
      " 'projects, make sure to detail those contributions.\\n'\n",
      " '\\n'\n",
      " '2. **Incorporate Key Responsibilities:**\\n'\n",
      " '   - Add specific examples of how you have developed or delivered products '\n",
      " 'focused on application security. Mention any innovative ideas or proof of '\n",
      " 'concepts you have created that align with security needs.\\n'\n",
      " '\\n'\n",
      " '3. **Showcase DevOps Experience:**\\n'\n",
      " '   - If you have experience with DevOps practices, particularly in '\n",
      " 'high-scalable enterprise applications, include that in your skills or '\n",
      " 'project descriptions.\\n'\n",
      " '\\n'\n",
      " '4. **Emphasize Cloud-Native Architecture:**\\n'\n",
      " '   - Clearly state your experience with cloud platforms (AWS, GCP) and any '\n",
      " 'relevant projects that demonstrate your hands-on experience in cloud-native '\n",
      " 'architecture.\\n'\n",
      " '\\n'\n",
      " '5. **Focus on AIML Security:**\\n'\n",
      " '   - If you have knowledge or experience in AIML security, web, API, or '\n",
      " 'mobile security, make sure to include that in your skills section or project '\n",
      " 'descriptions.\\n'\n",
      " '\\n'\n",
      " '6. **Leadership and Collaboration:**\\n'\n",
      " '   - Highlight any experience leading cross-functional teams or '\n",
      " 'collaborating with diverse groups, as this is a key requirement for the '\n",
      " 'role.\\n'\n",
      " '\\n'\n",
      " '7. **Tailor Your Summary:**\\n'\n",
      " '   - Revise your summary to reflect your passion for AI/ML security and your '\n",
      " 'commitment to aligning product innovation with cybersecurity best '\n",
      " 'practices.\\n'\n",
      " '\\n'\n",
      " '8. **Add Relevant Certifications:**\\n'\n",
      " '   - If you have any certifications related to security, AI/ML, or cloud '\n",
      " 'technologies, include them to strengthen your qualifications.\\n'\n",
      " '\\n'\n",
      " 'By making these adjustments, your resume will better reflect the '\n",
      " 'qualifications and responsibilities outlined in the job description, '\n",
      " 'increasing your chances of being considered for the position.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"question\": \"What chnages can I make to my resume to better fit the job description?\",\n",
    "    \"metadata_summary\": metadata_summary,\n",
    "    \"job_description\": jd_summary\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # print full state at each node\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5441cc1-0a17-41ba-bb5a-26dac9586260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
